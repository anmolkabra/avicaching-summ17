\documentclass[12pt]{article}

\usepackage{blindtext} % Package to generate dummy text throughout this template 

\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb}
\usepackage{algorithm}
\usepackage{algpseudocode}
%\usepackage{times} % Use the Times font
%\usepackage[T1]{fontenc}
\linespread{1.25}
\usepackage[margin = 1in]{geometry}
\usepackage{microtype} % Slightly tweak font spacing for aesthetics
\usepackage[english]{babel} % Language hyphenation and typographical rules
\usepackage{tikz}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{float}
\usepackage{wrapfig}

\setlength{\parindent}{2em}
%\setlength{\parskip}{1.5em}
\graphicspath{ {images/} }

\newcommand{\vect}[1]{\mathbf{#1}}  % vector
\newcommand{\matr}[1]{\mathbf{#1}}  % matrix
\newcommand{\tens}[1]{\mathbf{#1}}  % tensor
\newcommand{\mean}[1]{\overline{#1}}    % mean overline

\begin{document}
    \tableofcontents
    \listoftables
    \listoffigures
    \section{Introduction} \label{sec:introduction}
    Optimizing predictive models datasets obtained from citizen science projects can be computationally expensive due to limited parallelism offered by Central Processing Units (CPUs). Since these crowd-sourced datasets grow with time as the researchers warrant more information, running models can prove increasingly difficult even on the fastest CPUs available in the market. However, Graphical Processing Units (GPUs), which offer multiple cores to parallelize computation, can outperform CPUs in computing such predictive models given the operations are arithmetically simple (though large in number) and boolean logic-free to the maximum extent.\\
    
    One such predictive model is Avicaching \cite{Xue2016Avi1} in the eBird platform [cite], which aims to ``maximize the utility and accessibility of the vast numbers of bird observations made each year by recreational and professional bird watchers'' [cite website]. Since the eBird dataset is heterogeneous geographically, Avicaching tries to homogenize future observations by providing observer-citizens incentives to visit under-sampled locations \cite{Xue2016Avi1}. Therefore, the Avicaching game can be modeled as two separate problems - identifying parameters of citizens' behavior on a specific set of rewards (incentives), and pooling rewards from a budget to locations such that the citizens' predicted behavior is homogeneous \cite{Xue2016Avi2}.
    
    \subsection{Identification Problem} \label{sec:iden_problem}
    This subroutine will learn parameters of the change in citizens' behavior due to rewards. Specifically, given datasets $\vect{y_t}$ and $\vect{x_t}$ of citizens' visit densities with and without the rewards $\vect(r_t)$, we want to find weights $\matr{w}$ that caused the change from $\vect{x_t}$ to $\vect{y_t}$, factoring in possible influence due to environmental factors $\matr{f}$ and distances between locations $\matr{d}$. Therefore, the model is:
    \begin{equation} \label{eq:iden_problem}
    \begin{aligned}
    & \underset{\matr{w}}{\text{minimize}}
    & & Z_I(\matr{w}) = \sum_{t} (u_t(\vect{y_t} - \matr{P}(\matr{f}, \vect{r_t}; \matr{w})\vect{x_t}))^{2}
    \end{aligned}
    \end{equation}
    where $u_t$ is the total number of submissions at time $t$ for a given citizen and elements $p_{u, v}$ of $P$ are given as:
    \begin{equation} \label{eq:puv_equation}
    p_{u, v} = \frac{\exp{(\matr{w} \cdot [d_{u, v}, \vect{f_{u, v}}, r_{u}])}}{\sum_{u'} \exp{(\matr{w} \cdot [d_{u', v}, \vect{f_{u', v}}, r_{u'}]}}
    \end{equation}
    
    \subsection{Pricing Problem} \label{sec:pricing_problem}
    The Pricing subroutine will attempt to allocate rewards to all locations such that the predicted behavior is least heterogeneous, given the learned parameters from Section \ref{sec:iden_problem}. This optimization problem can be written as:
    \begin{equation} \label{eq:pricing_problem}
    \begin{aligned}
    & \underset{\vect{r}}{\text{minimize}}
    & & Z_P(\vect{r}) = \frac{1}{n}\lVert \vect{y} - \mean{\vect{y}} \rVert\\
    & \text{subject to}
    & & \vect{y} = \matr{P}(\matr{f}, \vect{r}; \matr{w}) \, \vect{x}\\
    &&& \sum_{i} r_i \leq \mathcal{R}\\
    &&& r_i \geq 0
    \end{aligned}
    \end{equation}
    where $\mathcal{R}$ is total reward budget and $\matr{P}$ is defined as in Equation \ref{eq:puv_equation}.
    
%    \begin{tikzpicture}[x=(15:.5cm), y=(90:.5cm), z=(330:.5cm), >=stealth]
%    \draw (0, 0, 0) -- (0, 0, 10) (4, 0, 0) -- (4, 0, 10);
%    \foreach \z in {0, 5, 10} \foreach \x in {0,...,3}
%    \foreach \y [evaluate={\b=random(0, 1);}] in {0,...,3}
%    \filldraw [fill=white] (\x, \y, \z) -- (\x+1, \y, \z) -- (\x+1, \y+1, \z) --
%    (\x, \y+1, \z) -- cycle (\x+.5, \y+.5, \z) node [yslant=tan(15)] {\b};
%    \draw [dashed] (0, 4, 0) -- (0, 4, 10) (4, 4, 0) -- (4, 4, 10);
%    \draw [->] (0, 4.5, 0)  -- (4, 4.5, 0)   node [near end, above left] {Column};
%    \draw [->] (-.5, 4, 0)  -- (-.5, 0, 0)   node [midway, left] {Row};
%    \draw [->] (4, 4.5, 10) -- (4, 4.5, 2.5) node [near end, above right] {Channel};
%    \end{tikzpicture}
%    
%    \tikzset{%
%        every neuron/.style={
%            circle,
%            draw,
%            minimum size=1cm
%        },
%        neuron missing/.style={
%            draw=none, 
%            scale=4,
%            text height=0.333cm,
%            execute at begin node=\color{black}$\vdots$
%        },
%    }
%    
%    \begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]
%    
%    \foreach \m/\l [count=\y] in {1,2,3,missing,4}
%    \node [every neuron/.try, neuron \m/.try] (input-\m) at (0,2.5-\y) {};
%    
%    \foreach \m [count=\y] in {1,missing,2}
%    \node [every neuron/.try, neuron \m/.try ] (hidden-\m) at (2,2-\y*1.25) {};
%    
%    \foreach \m [count=\y] in {1,missing,2}
%    \node [every neuron/.try, neuron \m/.try ] (output-\m) at (4,1.5-\y) {};
%    
%    \foreach \l [count=\i] in {1,2,3,n}
%    \draw [<-] (input-\i) -- ++(-1,0)
%    node [above, midway] {$I_\l$};
%    
%    \foreach \l [count=\i] in {1,n}
%    \node [above] at (hidden-\i.north) {$H_\l$};
%    
%    \foreach \l [count=\i] in {1,n}
%    \draw [->] (output-\i) -- ++(1,0)
%    node [above, midway] {$O_\l$};
%    
%    \foreach \i in {1,...,4}
%    \foreach \j in {1,...,2}
%    \draw [->] (input-\i) -- (hidden-\j);
%    
%    \foreach \i in {1,...,2}
%    \foreach \j in {1,...,2}
%    \draw [->] (hidden-\i) -- (output-\j);
%    
%    \foreach \l [count=\x from 0] in {Input, Hidden, Ouput}
%    \node [align=center, above] at (\x*2,2) {\l \\ layer};
%    
%    \end{tikzpicture}
    
    
    \subsection{Computation Using GPUs} \label{sec:comp_using_GPUs}
    
    \section{Problem Formulation}
    Since NVIDIA General Purpose GPUs enable faster computation on matrices, accelerated through CUDA and cuDNN, both the Identification Problem (Section \ref{sec:iden_problem}) and Pricing Problem (Section \ref{sec:pricing_problem}) were formulated as 2-layer Neural Networks using the PyTorch platform.
    
    \subsection{Design for Identification Problem}
    
    The Identification Problem (Section \ref{sec:iden_problem}) can be modeled as a typical 2-layered Neural Network, which learns the weights through multiple iterations of back-propagating the loss value $Z_I(\matr{w})$ (equation \ref{eq:iden_problem}) using gradient descent.
    
    \subsubsection{Structure of Input Dataset}
    \begin{figure}[h]
        \centering
        \includegraphics[width=.4\textwidth]{weights_input_dataset}
        \caption{A Tensor representing the Input Dataset}
        \label{fig:A Tensor representing the Input Dataset}
    \end{figure}
    
    \begin{wrapfigure}{r}{0.4\textwidth}
        \centering
        \includegraphics[width=0.3\textwidth]{zoomup_Fuv}
        \caption{Contents of $\vect{F}_{vu}$}
        \label{fig:Contents of Fvu}
    \end{wrapfigure}
    The input dataset comprising of environmental features $\vect{f}$ and given rewards $\vect{r_t}$ must be combined into a tensor (figure \ref{fig:A Tensor representing the Input Dataset}) that can be readily operated on by NVIDIA GPUs. Another advantage of building the dataset as discussed comes with the PyTorch library, which provides convenient handling of tensors residing on CPUs as well as GPUs. Algorithm \ref{alg:Constructing the Input Dataset} describes the steps to construct this dataset.
    
    \begin{algorithm}
        \caption{Constructing the Input Dataset} \label{alg:Constructing the Input Dataset}
        \begin{algorithmic}[1]
            \State $\matr{d} \gets \Call{Normalize}{\matr{d}}$\Comment{$\matr{d}[u][v]$ is the distance between locations $u$ and $v$}
            \State $\vect{f} \gets \Call{Normalize}{\mathbf{f}, axis = 0}$\Comment{$\mathbf{f}[u]$ is a vector of env. features at location $u$}
            \State $\vect{r_t} \gets \Call{Normalize}{\vect{r_t}, axis = 0}$\Comment{$\vect{r_t}[u]$ is the reward at location $u$}
            \For{$v = 1, 2, \dots J$}
                \For{$u = 1, 2, \dots J$}
                    \State $\tens{F}[v][u] \gets [\matr{d}[v][u], \vect{f}[u], \vect{r_t}[u]]$ \ref{fig:Contents of Fvu}
                \EndFor
            \EndFor
        \end{algorithmic}
    \end{algorithm}
       
    \subsubsection{Algorithm for Identification Problem}
    For learning the weights $\matr{w}$ in equation \ref{eq:iden_problem}, a Neural Network was built using the PyTorch library, which minimized the loss function using gradient descent routines. Figure \ref{fig:Neural Network designed for Identification Problem} illustrates the framework of the network used, calculating the vector $\vect{p_v}$. This network was fed in with data from different locations $v$, resulting into the matrix $\matr{P}$, which was then used to calculate the loss.
    \begin{figure}[H]
        \centering
        \includegraphics[width=\textwidth]{weights_net}
        \caption{Neural Network designed for Identification Problem}
        \label{fig:Neural Network designed for Identification Problem}
    \end{figure}
    An algorithm for the optimization problem is described in Algorithm \ref{alg:Optimizing for the Identification Problem}.
    \begin{algorithm}
        \caption{Optimizing for the Identification Problem} \label{alg:Optimizing for the Identification Problem}
        \begin{algorithmic}[1]
            \State $a \gets b$
        \end{algorithmic}
    \end{algorithm}
    \section{Experiments}
    
    \section{Results}
    \section{Conclusion}
    
    \blindtext
    \bibliographystyle{ieeetr}
    \bibliography{avicaching}
\end{document}