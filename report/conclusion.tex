\chapter{Conclusion} \label{sec:Conclusion}
Our models for the Identification and the Pricing Problem outperformed previously studied ones \cite{Xue2016Avi2} and other baseline comparisons (\Cref{sec:IdProbRes - Optimization,sec:PriProbRes - Optimization,tab:Loss Values Calculated for Different Models for Identification Problem,tab:Loss Values Calculated from Different Sets of Rewards}). For the Identification Problem, the average loss value was $14\%$ lower than the previous 2-layered model, and $12\%$ better than the 4-layered model, giving us better results than any other tested model. While we did not test deeper networks, we contend that using more hidden layers will only aggravate overfitting and won't provide better results - as is partly the case with the 4-layered network. The Pricing Problem's model also delivered at least $3\times$~lower loss values than other baseline comparisons for reward distribution.

On the other hand, we can definitively conclude that the Identification Problem ran $\approx 9\times$~faster on the GPU than the CPU, mainly because the model was based on tensors and a neural network, accelerated by a GPU and NVIDIA's APIs. With an approximate GPU Speedup of $9.06$ for the Identification Problem, we can scale to large datasets more efficiently on the GPU than the CPU. The Pricing Problem's neural network only performed better with higher batch-sizes, with transfer times hampering performance on smaller datasets. Although the Pricing Problem's full model did not deliver a noticeable speedup (with the LP problem heavily impacting the runtime), the 2-layered network for finding rewards gave a speedup of $1.41\pm0.76$ (mean over all tested batch-sizes). This shows that neural network are inherently quick to optimize on a GPU only if the batch-sizes are large enough. This issue of high transfer times is also discussed in \Cref{sec:Computation Using GPUs} and recent literature \cite[Appendix~B]{PattersonARM}, inferring that one can witness GPU Speedups only when transfer time are tiny compared to computation time. Using faster GPUs like NVIDIA Quadro GP100, and those based on the newer Volta architecture will most likely decrease the models' runtimes. There still exists enormous scope for improvement on both fronts -- optimized results and faster computation.

\section{Interesting Inferences}
One may also notice compelling reflections from the results:
\begin{itemize}
    \item One interesting observation in \Cref{tab:Loss Values Calculated from Different Sets of Rewards} is that the loss value from the Proportional Distribution ($0.0235\%$) and Random Initialization ($0.0331\%$) are very close, highlighting that the set of weights obtained from the Identification Problem are dependent on other factors ($\matr{f}, \matr{D}$) as well. In other words, only incentivizing under-sampled locations more is as good as random distribution of rewards -- as agents consider environmental features and distances between locations to make decisions as well.
    
    \item By looking at the model's generated rewards (\Cref{tab:Example Rewards Prediction by the Pricing Problem's Model}), one can infer that the model chooses to place large rewards in very under-sampled locations, rather than distributing more evenly over all locations. Although this non-uniform distribution is unintuitive to the human perspective, it gives much lower results than proportionally allocated rewards.
    
    \item Algorithms that use different libraries for sub-routines' implementation can have difficulty in parallelizing. As we saw with the unexpected GPU Speedup in LP runtimes in the Pricing Problem (\Cref{sec:PriProbRes - GPU,app:GPU Speedup in LP Computation}), na\"ive synchronization barriers can affect performance in CPUs' multi-threading mechanism as well as GPUs' threads, if the logic requires branching (\Cref{sec:Avoiding Specific Operations in GPUs}). Therefore, one might consider threading the \textit{full} algorithm instead of using external libraries for threading some sub-routines.
    
    \item GPUs are not always effective in executing all kinds of models. Factors that affect the performance include the size of the model and datasets, extent of conditional statements in the program, requirement of synchronization barriers, data transfer between RAM and GPU, algorithm's `parallel-friendliness' and many more. Reducing model complexity always helps, but when the models are increasingly simple and small, CPUs can do a much better job than GPUs.
\end{itemize}
\begin{table}[!htbp]
    \centering
    \caption[Example Rewards Prediction by the Pricing Problem's Model]{Example Rewards Prediction by the Pricing Problem's Model: The prediction is relatively sparse and non-uniform. Parameters: $\mathcal{R} = 1000$, Loss value = $0.0068\%$, Epochs = 1000, Learning Rate = $5 \times 10^{-5}$, Weights: Set-2. (values rounded)}
    \label{tab:Example Rewards Prediction by the Pricing Problem's Model}
    \setlength\tabcolsep{2pt}
    \begin{tabular}{|*{15}{c}|}
        \hline
        0.00 & 38.02 & 0.00 & 0.00 & 0.00 & 0.00 & 14.12 & 0.00 & 24.63 & 4.43 & 3.18 & 24.35 & 0.00 & 19.53 & 0.00\\
        1.63 & 6.40 & 0.00 & 0.00 & 0.00 & 36.31 & 31.15 & 0.00 & 0.00 & 29.16 & 2.22 & 6.02 & 23.12 & 0.00 & 16.42\\
        0.00 & 0.00 & 28.45 & 0.00 & 37.50 & 0.00 & 20.04 & 34.19 & 0.00 & 0.00 & 18.29 & 0.00 & 0.00 & 21.33 & 0.00\\
        0.00 & 23.73 & 24.77 & 0.00 & 23.18 & 1.75 & 0.00 & 20.78 & 22.60 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 4.79\\
        4.67 & 0.00 & 0.00 & 34.82 & 0.00 & 9.47 & 0.00 & 0.00 & 31.43 & 0.00 & 4.35 & 16.55 & 28.51 & 6.02 & 0.00\\
        21.24 & 21.38 & 0.00 & 22.95 & 27.17 & 21.44 & 24.16 & 21.07 & 0.00 & 0.00 & 25.48 & 0.00 & 0.00 & 0.00 & 2.59\\
        12.28 & 0.00 & 0.00 & 0.00 & 0.00 & 0.00 & 22.09 & 0.00 & 0.00 & 16.50 & 0.00 & 2.88 & 1.57 & 0.00 & 43.45\\
        0.00 & 0.00 & 15.25 & 0.02 & 0.00 & 0.00 & 0.00 & 0.00 & 0.18 & 0.32 & 0.00&&&&\\ \hline
    \end{tabular}
\end{table}

\section{Limitations} \label{sec:Limitations}
We could not parallelize some sub-routines of \Cref{alg:Algorithm for the Identification Problem,alg:Solving the Pricing Problem} and had to rely on PyTorch's implementation for the most part. While PyTorch's implementation \cite{PTDocs} may be very efficient, we do not know if it suits our models best -- a typical problem when using external libraries.

On the other hand, we did not venture in adjusting the model's parameters much as the models took long time to tests, and there lied multiple possibilities. Tinkering with the models' characteristics might have optimized the models more if we had devised a way to experiment efficiently. This also remains a topic for future study.

We also could not venture in-depth of quirky behavior and ascertain reasons with high confidence. Our research project limited us to focus on improving the Avicaching game, restricting available time to rigorously study thread synchronization delays and GPU's architecture.

\section{Further Research}
There exist numerous possibilities for solving the problems better and faster -- from more complex models to better preprocessing to more parallel algorithms with fewer synchronization barriers. Some important suggestions are listed below:

\subsubsection{Choice of Gradient-Descent Algorithm}
\Cref{fig:Plot for 3-layered Model} shows how the choice of Adam's algorithm \cite{Adam} for \textsc{Gradient-Descent}($\cdot$) helps the model to learn quickly. However, we also witness long periods of saturation after a few epochs. This was the case for several other algorithms (SGD \cite{SGD} and Adagrad \cite{Adagrad}) as well, but with different paces of learning. Since the organizers would want to further optimize the set of weights, research could be done on avoiding the long, unwavering saturation phase. This may involve using other algorithms for \textsc{Gradient-Descent}($\cdot$) (\Cref{alg:Algorithm for the Identification Problem}) and/or altering the loss function ($Z_P$ -- \Cref{eqn:iden_problem}).

\subsubsection{Modeling LP Differently to Reduce Runtimes}
LP is a simple tool for optimizing different problems, with various algorithms for solving LPs - Simplex, Criss-Cross and other Interior Point techniques. While it gives optimal results, it can be computationally expensive if the matrices are large (as depicted in \Cref{fig:Restricted Finding Rewards - Time taken by the LP}). One can try several approaches to reduce computation time here:
\begin{itemize}
    \item Implement GPU or OpenMP backend for the LP.
    \item Constrain Rewards differently (\Cref{sec:Constraining Rewards}). Parallelized algorithms for constraining rewards can decrease the runtime for the Pricing Problem's model considerably as the current LP accounts for $\approx 94\%$ of the total runtime.
\end{itemize}
