\chapter{Results} \label{sec:Results}
\section{Identification Problem's Results} \label{sec:Identification Problem's Results}
\subsection{Optimization Results} \label{sec:IdProbRes - Optimization}
Running the 3-layered network with the GPU ``set'' with different learning rates on different seeds for 10000 epochs showed that the model was constantly giving lower loss values (\Cref{eqn:iden_problem}) with learning rate = $10^{-3}$. 

We observed that higher learning rates ($>10^{-2}$) could only decrease the loss value to a limit, after which the updates in the weights caused the loss function to oscillate and increase. On the other side of $10^{-3}$, lower learning rates took too long to train. This phenomena is exemplified in \Cref{fig:Test Loss Plots of Different Learning Rates to Find Weights}, which shows plots of different runs with the same seed but different learning rates. Runtime for the model on 10000 epochs was 1232.56 seconds (average over 20 runs = 5 seeds $\times$ 4 learning rates), and models with learning rates less than $10^{-3}$ did not perform better than those with learning rate = $10^{-3}$ on \textit{any} run. Although there was constant, gradual decrease in test losses for learning rates less than $10^{-3}$, we argue that models with those specifications may be computationally expensive and temporally inconvenient to train.
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=16cm, height=10cm]{test_losses_diff_lr_plot}
    \caption[Test Loss Plots of Different Learning Rates to Find Weights]{Test Loss Plots of Different Learning Rates to Find Weights: Learning rates lower than $10^{-3}$ couldn't reach the loss value attained by the model at learning rate = $10^{-3}$ even after 10000 epochs.}
    \label{fig:Test Loss Plots of Different Learning Rates to Find Weights}
\end{figure}
\begin{figure}[!htbp]
    \centering
    \begin{minipage}{.48\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
        ybar,
        width=.98\textwidth,
        enlarge x limits=0.15,
        legend style={at={(0.02,0.98)},
            anchor=north west,legend columns=1},
        ylabel={Loss Value},
        symbolic x coords={2-layer,3-layer,4-layer, Historical},
        xtick=data,
        nodes near coords,
        nodes near coords align={vertical},
        every node near coord/.append style={/pgf/number format/fixed},
        xlabel={Models},
        ]
        \addplot coordinates {(2-layer,0.26) (3-layer,0.12) (4-layer,0.23) (Historical, 0.36)};% test set
        \addplot coordinates {(3-layer,0.03) (4-layer,0.06)};% training set
        \legend{Test Set,Training Set}
        \end{axis}
        \end{tikzpicture}
        \caption[Comparison of Loss Values from Different Models of the Identification Problem]{Comparison of Loss Values from Different Models of the Identification Problem: Loss values for the training set are inevitably lower than that for the test set, which is the basis for comparison.}
        \label{fig:Comparison of Loss Values from Different Models of the Identification Problem}
    \end{minipage}\hfill
    \begin{minipage}{.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{weights_map_plot_3}
        \caption[Predicted Probabilities of Agents Visiting Each Location Plotted on a Map (Latitude, Longitude) Representing Tompkins and Cortland Counties, NY]{Predicted Probabilities of Agents Visiting Each Location Plotted on a Map (Latitude, Longitude) Representing Tompkins and Cortland Counties, NY: Dark dots represent high prediction of visits. This can be compared to the plots for the 2-layered network and other models \cite[Figure~3]{Xue2016Avi2}.}
        \label{fig:Predicted Probabilities of Agents Visiting Each Location Plotted on a Map}
    \end{minipage}
\end{figure}

\begin{table}[!htbp]
    \centering
    \caption[Loss Values Calculated for Different Models for Identification Problem]{Loss Values Calculated for Different Models for Identification Problem: For both the 3- and the 4-layered models, learning rate = $10^{-3}$ outperformed other learning rates. Consequently, that learning rate is used in comparison with other models in \Cref{fig:Comparison of Loss Values from Different Models of the Identification Problem}.}
    \label{tab:Loss Values Calculated for Different Models for Identification Problem}
    \begin{tabular}{c | c | c}
        \hline
        \multirow{2}{*}{\textbf{Learning Rate}} & \multicolumn{2}{c}{\textbf{Average Test Loss Values}} \\
        & 3-layered & 4-layered\\
        \hline
        $10^{-2}$ & 0.168 $\pm$ 0.068 & 0.494 $\pm$ 0.083\\
        $10^{-3}$ & \textbf{0.119 $\pm$ 0.016} & \textbf{0.228 $\pm$ 0.048}\\
        $10^{-4}$ & 0.151 $\pm$ 0.040 & 0.237 $\pm$ 0.067\\
        $10^{-5}$ & 0.212 $\pm$ 0.040 & 0.320 $\pm$ 0.067\\
        \hline
    \end{tabular}
\end{table}

Observing that the average test loss values of learning rate = $10^{-3}$ is the lowest, we compare its results with the previous study's 2-layered network, historical data \cite[Table~1]{Xue2016Avi2}, and the 4-layered network with learning rate = $10^{-3}$ (see \Cref{sec:Identification Problem-Optimizing the Original Dataset}). As depicted in \Cref{fig:Comparison of Loss Values from Different Models of the Identification Problem}, our 3-layered neural network outperformed the 2-layered model by \textbf{0.14~units}~in~loss~value, and also produced better results (0.12~units less) than the 4-layered model.

We also generated the predicted probabilities of the agents visiting each location ($\matr{P}\matr{x}$) during time units in the Test Set, and plotted it onto maps marked by the locations' latitudes and longitudes. \Cref{fig:Predicted Probabilities of Agents Visiting Each Location Plotted on a Map} shows such a plot generated by the 3-layered network, where each dot represents a location.
\begin{figure}[!htbp]
    \centering
    \begin{subfigure}{.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{weights_train_test_loss_3_plot}
        \caption{Plot for 3-layered Model}
        \label{fig:Plot for 3-layered Model}
    \end{subfigure}
    \begin{subfigure}{.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{weights_train_test_loss_4_plot}
        \caption{Plot for 4-layered Model // todo}
        \label{fig:Plot for 4-layered Model}
    \end{subfigure}
    \caption[Train and Test Loss Values' Plots of Different Models]{Train and Test Loss Values' Plots of Different Models: Both networks learn the set of weights quickly, as displayed in the steep descent in loss values before $\approx 1000$ epochs. This quick learning is due to the choice of \textsc{Gradient-Descent}($\cdot$) function --- Adam's algorithm \cite{Adam}. Other algorithms like SGD \cite{SGD} and Adagrad \cite{Adagrad} learn relatively slowly.}
    \label{fig:Train & Test Loss Values' Plots of Different Models}
\end{figure}

To check if the model was overfitting at learning rate = $10^{-3}$, we plotted loss values at the end of each epoch for all tests. Although there remained a difference of 0.09 loss units in the values of training and testing set, the 3-layered model was not drastically overfitting as an average \textit{end} difference of $0.0876 \pm 0.0159$ loss units persisted for many epochs, instead of increasing and tuning more to the training set. 

Even though one would expect overfitting with more tuned parameters, the 4-layered model also  produced an average \textit{end} difference of $0.01677 \pm 0.0473$ loss units, which persisted during the run. \Cref{fig:Train & Test Loss Values' Plots of Different Models} shows the results of a randomly selected experiment with plots of loss values at each epoch for the both networks.

\subsection{GPU Speedup Results} \label{sec:IdProbRes - GPU}
Running on batches of sizes $J = \{11,37,63,90,116,145,174,203,232\}$ on a randomly generated dataset for 1000 epochs with both GPU and CPU ``set'' separately, we obtained information on full execution runtimes (combining training and testing runtimes). The average results (over 3 seeds for each batch) are plotted in \Cref{fig:Execution Times of Different Batch-Sizes with GPU and CPU ``set'' Separately}, which show promising GPU Speedup figures for bigger dataset batches; the GPU Speedup varied \textbf{from~0.94~to~10.32} on all tested batch-sizes.
\begin{figure}[!htbp]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
        width=.9\textwidth,
        height=8cm,
        xtick=data,
        xlabel={Batch-Size $J$},
        ylabel={Time/s},
        enlarge x limits=0.05,
        enlarge y limits=0.15,
        y tick label style={/pgf/number format/1000 sep=},
        extra y tick style={grid=major, tick label style={xshift=-1cm}},
        legend style={at={(0.02,.98)},
            anchor=north west},
        nodes near coords,
        every node near coord/.append style={yshift=-0.7cm,/pgf/number format/.cd,fixed zerofill,precision=2},
        ]
        \addplot+ [mark=*,
        nodes near coords=\raisebox{0.8cm}{\pgfmathprintnumber\pgfplotspointmeta},error bars/.cd, y dir=both,y explicit] coordinates {
            (11,95.32) +- (1.62,1.62)
            (37,202.10) +- (4.48,4.48)
            (63,391.45) +- (2.82,2.82)
            (90,654.24) +- (0.41,0.41)
            (116,999.53) +- (1.98,1.98)
            (145,1248.27) +- (5.50,5.50)
            (174,1748.61) +- (1.53,1.53)
            (203,2279.90) +- (1.18,1.18)
            (232,2777.15) +- (36.84,36.84)
        };  % cpu
        \addplot+ [mark=*,error bars/.cd, y dir=both,y explicit] coordinates {
            (11,100.77) +- (0.74,0.74)
            (37,101.09) +- (0.38,0.38)
            (63,101.55) +- (0.76,0.76)
            (90,100.62) +- (0.33,0.33)
            (116,109.77) +- (0.07,0.07)
            (145,137.35) +- (0.06,0.06)
            (174,169.53) +- (0.16,0.16)
            (203,224.14) +- (0.14,0.14)
            (232,269.20) +- (0.11,0.11)
        };  % gpu
        \legend{CPU ``set'',GPU ``set''}
        \end{axis}
        \end{tikzpicture}
        \caption{Time Taken by the Full Model: GPUs can sustain calculations without increase in runtime for small datasets (upto $J=116$ in this case). This may be because of underutilization of GPU resources for small datasets (fewer threads needed). On the other hand, full utilization results in a steady increase in runtime, as seen in the upward slope trend after $J=116$.}
    \end{subfigure}\vspace*{1em}
    \begin{subfigure}{\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
            xbar,
            width=.9\textwidth,
            height=8cm,
            ytick=data,
            ylabel={Batch-Size $J$},
            xlabel={GPU Speedup},
            enlarge y limits=0.15,
            nodes near coords,
            every node near coord/.append style={/pgf/number format/.cd,fixed zerofill,precision=2},
            ]
            \addplot+ coordinates {
                (0.94,11)
                (2.00,37)
                (3.85,63)
                (6.50,90)
                (9.10,116)
                (9.09,145)
                (10.31,174)
                (10.17,203)
                (10.32,232)
            };
            \legend{}
            \end{axis}
        \end{tikzpicture}
        \caption{GPU Speedup for the Identification Problem: We see the GPU performing operations much faster for bigger datasets. The absence of drastic speedup for smaller datasets highlights the inferiority of GPUs for tiny models, which is often caused due to dominating transfer times.}
    \end{subfigure}
    \caption[Finding Weights --- Execution Times of Different Batch-Sizes $J$ with GPU and CPU ``set'' Separately]{Finding Weights --- Execution Times of Different Batch-Sizes $J$ with GPU and CPU ``set'' Separately: The GPU delivers faster computation than CPU even as the datasets' sizes grow --- the speedup is $0.94-10.32$ depending on the tested dataset's size. Also, the error bars are indiscernible because they are too small ($< 1\%$).}
    \label{fig:Execution Times of Different Batch-Sizes with GPU and CPU ``set'' Separately}
\end{figure}

\section{Pricing Problem's Results} \label{sec:Pricing Problem's Results}
\subsection{Optimization Results} \label{sec:PriProbRes - Optimization}
Taking the approach mentioned in \Cref{sec:Pricing Problem-Optimizing the Original Dataset}, we obtained consistent loss values for each set of weights (even with differently seeded rewards). The best performing set of weights was set-2, using which the average loss value for the Pricing Problem hovered around $0.0079\%$. Next, running differently seeded rewards with different learning rates on set-2 of weights, we obtained the lowest loss value of $\textbf{0.0069\%}$.
\begin{table}[!htbp]
    \centering
    \caption[Loss Values Calculated from Different Sets of Rewards]{Loss Values Calculated from Different Sets of Rewards: The values are small because the loss function $Z_P(\vect{r})$ (\Cref{eqn:pricing_problem}) is averaged over the number of locations.}
    \label{tab:Loss Values Calculated from Different Sets of Rewards}
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Rewards Obtained From} & \textbf{Best Loss Values (In \%)}\\
        \hline
        Model's Prediction & 0.0069\\
        Random Initialization & 0.0331\\
        Proportional Distribution & 0.0235\\
        \hline
    \end{tabular}
\end{table}

Compared to the proportional reward distribution (loss values calculated using set-2 of weights), our model's set of rewards produced loss value $\approx$ \textbf{3 times} lower. \Cref{tab:Loss Values Calculated from Different Sets of Rewards} lists the best loss values obtained on each type of reward allocation (model's predicted, random and proportional --- \Cref{sec:Pricing Problem-Optimizing the Original Dataset}).

\subsection{GPU Speedup Results} \label{sec:PriProbRes - GPU}
After running on different batch-sizes $J = \{11, 37, 63, 90, 116, 145, 174, 203, 232\}$, we \textbf{did not observe drastic GPU speedup} for the full model. \Cref{fig:Finding Rewards - Time Taken by the Full Model} shows the Speedup trend: GPU Speedup for the full model was a meager \textbf{1.12 $-$ 1.60}.
\begin{figure}[!htbp]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
            width=.9\textwidth,
            height=8cm,
            xtick=data,
            xlabel={Batch-Size $J$},
            ylabel={Time/s},
            enlarge y limits=0.15,
            enlarge x limits=0.05,
            y tick label style={/pgf/number format/1000 sep=},
            extra y tick style={grid=major, tick label style={xshift=-1cm}},
            legend style={at={(0.02,.98)},
                anchor=north west},
            nodes near coords,
            every node near coord/.append style={yshift=-0.7cm,/pgf/number format/.cd,fixed zerofill,precision=2},
            ]
            \addplot+ [mark=*,
            nodes near coords=\raisebox{0.8cm}{\pgfmathprintnumber\pgfplotspointmeta},error bars/.cd, y dir=both,y explicit
            ] coordinates {
                (11,9.95) +- (0.12,0.12)
                (37,41.70) +- (0.18,0.18)
                (63,83.69) +- (1.60,1.60)
                (90,144.59) +- (1.37,1.37)
                (116,234.69) +- (2.09,2.09)
                (145,299.91) +- (2.46,2.46)
                (174,390.79) +- (3.49,3.49)
                (203,510.15) +- (4.29,4.29)
                (232,651.64) +- (5.25,5.25)
            };  % cpu
            \addplot+ [mark=*,error bars/.cd, y dir=both,y explicit] coordinates {
                (11,8.79) +- (0.07,0.07)
                (37,29.43) +- (0.28,0.28)
                (63,55.59) +- (0.20,0.20)
                (90,94.10) +- (0.85,0.85)
                (116,146.63) +- (0.42,0.42)
                (145,223.77) +- (2.57,2.57)
                (174,323.67) +- (9.67,9.67)
                (203,432.95) +- (6.76,6.76)
                (232,581.69) +- (15.66,15.66)
            };  % gpu
            \legend{CPU ``set'',GPU ``set''}
            \end{axis}
        \end{tikzpicture}
        \caption{Time Taken by the Full Model: GPU Speedup over all batch-sizes is only \textbf{1.12 $-$ 1.60}.}
        \label{fig:Finding Rewards - Time Taken by the Full Model}
    \end{subfigure}\vspace*{1em}
    \begin{subfigure}{\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
            width=.9\textwidth,
            height=8cm,
            xtick=data,
            enlarge y limits=0.15,
            enlarge x limits=0.05,
            xlabel={Batch-Size $J$},
            ylabel={Time/s},
            y tick label style={/pgf/number format/1000 sep=},
            legend style={at={(0.02,.98)},
                anchor=north west},
            nodes near coords,
            every node near coord/.append style={yshift=-0.7cm,/pgf/number format/.cd,fixed zerofill,precision=2},
            ]
            \addplot+ [mark=*,
            nodes near coords=\raisebox{0.8cm}{\pgfmathprintnumber\pgfplotspointmeta},error bars/.cd, y dir=both,y explicit
            ] coordinates {
                (11,9.14) +- (0.03,0.03)
                (37,40.15) +- (0.18,0.18)
                (63,81.03) +- (1.56,1.56)
                (90,140.40) +- (1.37,1.37)
                (116,228.46) +- (2.14,2.14)
                (145,289.98) +- (2.38,2.38)
                (174,379.11) +- (3.44,3.44)
                (203,494.61) +- (4.16,4.16)
                (232,631.07) +- (5.32,5.32)
            };  % cpu
            \addplot+ [mark=*,error bars/.cd, y dir=both,y explicit] coordinates {
                (11,6.50) +- (0.07,0.07)
                (37,26.82) +- (0.28,0.28)
                (63,52.63) +- (0.20,0.20)
                (90,90.71) +- (0.80,0.80)
                (116,142.91) +- (0.42,0.42)
                (145,219.35) +- (2.72,2.72)
                (174,318.93) +- (9.69,9.69)
                (203,427.58) +- (6.75,6.75)
                (232,574.45) +- (16.02,16.02)
            };  % gpu
            \legend{CPU ``set'',GPU ``set''}
            \end{axis}
        \end{tikzpicture}
        \caption[Time taken by the LP]{Time taken by the LP: GPU Speedup over all batch-sizes varies \textbf{1.10 $-$ 1.60}. As discussed in \Cref{sec:PriProbRes - GPU,app:GPU Speedup in LP Computation}, we shouldn't have witnessed this speedup as both configurations' LPs were computed on the CPU. Hence, we expected the GPU Speedup value for LP to be $\approx$ 1. }
        \label{fig:Finding Rewards - Time taken by the LP}
    \end{subfigure}
    \caption{Finding Rewards --- Execution Times of Different Batch-Sizes $J$ with GPU and CPU ``set'' Separately}
    \label{fig:Finding Rewards - Execution Times of Different Batch-Sizes J with GPU and CPU ``set'' Separately}
\end{figure}

As the the low GPU Speedup was disappointing, we looked for operations that were causing the program to slow down on the GPU. Since the 2-layered network was quite small, we suspected the LP problem (\Cref{eqn:lp_math_constrain_rewards,eqn:lp_code_constrain_rewards}) to influence the runtimes. Thus, we recorded execution times for both the neural network and the LP separately. As we suspected, the LP did impact the runtime more than the neural networks did, and accounted for $\approx 94\%$ of the total model runtime (\Cref{fig:Finding Rewards - Execution Times of Different Batch-Sizes J with GPU and CPU ``set'' Separately}). However, the GPU Speedup in LP runtime was unusual.

\subsubsection{Strange GPU Speedup in LP Computation}
The Speedup is exceptional as we intentionally transferred the needed matrices/tensors to the CPU for solving the LP using Simplex. Since SciPy's Optimize Module uses a single CPU core and does not utilize the GPU, we expected similar runtimes for both configurations. Our efforts to determine the reasons for this speedup are collected in \Cref{app:GPU Speedup in LP Computation} instead of digressing here. As stated later in this section, we disregard the old GPU Speedup results for finding rewards and use the more suited ones.\\

Since \Cref{alg:Solving the Pricing Problem} performs operations sequentially (Calculate loss $\rightarrow$ Gradient-Descent and Update $\vect{r}$ $\rightarrow$ Constrain using LP), we suggest these possibilities:
\begin{enumerate}
    \item The 2-layered network is parallelized but \texttt{scipy.optimize}'s LP solver is not. Since both sub-problems are solved by independent frameworks, we could not thread the whole algorithm without implementing OpenMP \cite{OpenMP} back-end manually and allowing multiple copies of the script to run simultaneously. Currently, the LP problem acts as a shared resource and requires all neural network's threads to synchronize. This might cause time lag as the LP waits for all elements/matrices to become fully available.
    \item The simultaneously threads downgrade the LP solver's performance. We do not endorse this possibility because the CPU resources are independent when using multi-threading over CPU's cores. We checked if the same script running simultaneously on another core hampered the performance of the original copy, but did not witness any reduction.
\end{enumerate}

We found that the LP in CPU ``set'' was slower than normal because of latency in thread synchronization\footnote{Pytorch uses OpenMP \cite{PTDocs,OpenMP} when the CPU is ``set'', which threads the 2-layered network.} (\Cref{app:Parallelism Doesn't Always Help}). While one might contend the lack of this behavior in GPU ``set'', we reiterate that we intentionally impose a synchronization barrier when transferring the matrices to the CPU after the network has been executed, adding the time elapsed in the network's time logs. This is not case with CPU ``set'' because the network remains in execution in other cores while the LP has started. Therefore, when we restrict the script's access to a single CPU core, we do not witness any GPU Speedup in LP runtime. This only affects CPU ``set'' performance because the GPU ``set'' configuration independently executes the network on the GPU.
\begin{figure}[!htbp]
    \centering
    \begin{subfigure}{\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
        width=.9\textwidth,
        height=8cm,
        xtick=data,
        xlabel={Batch-Size $J$},
        ylabel={Time/s},
        enlarge y limits=0.15,
        enlarge x limits=0.05,
        y tick label style={/pgf/number format/1000 sep=},
        extra y tick style={grid=major, tick label style={xshift=-1cm}},
        legend style={at={(0.02,.98)},
            anchor=north west},
        nodes near coords,
        every node near coord/.append style={yshift=-0.7cm,/pgf/number format/.cd,fixed zerofill,precision=2},
        ]
        \addplot+ [mark=*,
        nodes near coords=\raisebox{0.8cm}{\pgfmathprintnumber\pgfplotspointmeta},error bars/.cd, y dir=both,y explicit
        ] coordinates {
            (11,7.03) +- (0.03,0.03)
            (37,28.25) +- (0.28,0.28)
            (63,54.18) +- (0.29,0.29)
            (90,93.34) +- (0.39,0.39)
            (116,147.15) +- (0.42,0.42)
            (145,225.92) +- (1.74,1.74)
            (174,322.61) +- (4.21,4.21)
            (203,438.17) +- (1.56,1.56)
            (232,583.95) +- (5.08,5.08)
        };  % cpu
        \addplot+ [mark=*,error bars/.cd, y dir=both,y explicit] coordinates {
            (11,8.52) +- (0.02,0.02)
            (37,29.64) +- (0.16,0.16)
            (63,55.25) +- (0.05,0.05)
            (90,94.02) +- (0.87,0.87)
            (116,146.52) +- (0.50,0.50)
            (145,218.42) +- (0.81,0.81)
            (174,317.33) +- (3.98,3.98)
            (203,430.61) +- (2.55,2.55)
            (232,573.49) +- (4.69,4.69)
        };  % gpu
        \legend{CPU ``set'',GPU ``set''}
        \end{axis}
        \end{tikzpicture}
        \caption{Time Taken by the Full Model: GPU Speedup over all batch-sizes is only \textbf{0.82 $-$ 1.03} --- heavily impacted by LP runtimes.}
        \label{fig:Restricted Finding Rewards - Time Taken by the Full Model}
    \end{subfigure}\vspace*{1em}
    \begin{subfigure}{\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
        width=.9\textwidth,
        height=8cm,
        xtick=data,
        enlarge y limits=0.15,
        enlarge x limits=0.05,
        xlabel={Batch-Size $J$},
        ylabel={Time/s},
        y tick label style={/pgf/number format/1000 sep=},
        legend style={at={(0.02,.98)},
            anchor=north west},
        nodes near coords,
        every node near coord/.append style={yshift=-0.7cm,/pgf/number format/.cd,fixed zerofill,precision=2}
        ]
        \addplot+ [mark=*,nodes near coords=\raisebox{0.8cm}{\pgfmathprintnumber\pgfplotspointmeta},error bars/.cd, y dir=both,y explicit
        ] coordinates {
            (11,6.42) +- (0.03,0.03)
            (37,27.16) +- (0.28,0.28)
            (63,52.34) +- (0.28,0.28)
            (90,90.18) +- (0.35,0.35)
            (116,142.46) +- (0.42,0.42)
            (145,218.63) +- (1.72,1.72)
            (174,313.16) +- (4.22,4.22)
            (203,425.66) +- (1.56,1.56)
            (232,567.86) +- (5.10,5.10)
        };  % cpu
        \addplot+ [mark=*,error bars/.cd, y dir=both,y explicit] coordinates {
            (11,6.37) +- (0.01,0.01)
            (37,27.15) +- (0.16,0.16)
            (63,52.42) +- (0.06,0.06)
            (90,90.81) +- (0.87,0.87)
            (116,142.90) +- (0.51,0.51)
            (145,214.30) +- (0.81,0.81)
            (174,312.71) +- (3.98,3.98)
            (203,425.38) +- (2.58,2.58)
            (232,567.77) +- (4.71,4.71)
        };  % gpu
        \legend{CPU ``set'',GPU ``set''}
        \end{axis}
        \end{tikzpicture}
        \caption[Time taken by the LP]{Time taken by the LP: GPU Speedup over all batch-sizes is $\approx$ \textbf{1}. Expectedly, there is no GPU Speedup in LP runtimes.}
        \label{fig:Restricted Finding Rewards - Time taken by the LP}
    \end{subfigure}
\end{figure}

\begin{figure}[!htbp]
    \centering
    \ContinuedFloat
    \begin{subfigure}{\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
        width=.9\textwidth,
        height=9cm,
        xtick=data,
        enlarge y limits=0.15,
        enlarge x limits=0.05,
        xlabel={Batch-Size $J$},
        ylabel={Time/s},
        legend style={at={(0.02,.98)},
            anchor=north west},
        nodes near coords,
        every node near coord/.append style={yshift=-1.25cm,/pgf/number format/.cd,fixed zerofill,precision=2}
        ]
        \addplot+ [mark=*,
        nodes near coords=\raisebox{1.8cm}{\pgfmathprintnumber\pgfplotspointmeta},error bars/.cd, y dir=both,y explicit] coordinates {
            (11,0.61) +- (0.01,0.01)
            (37,1.09) +- (0.003,0.003)
            (63,1.84) +- (0.01,0.01)
            (90,3.16) +- (0.04,0.04)
            (116,4.69) +- (0.004,0.004)
            (145,7.28) +- (0.02,0.02)
            (174,9.45) +- (0.02,0.02)
            (203,12.51) +- (0.06,0.06)
            (232,16.08) +- (0.04,0.04)
        };  % cpu
        \addplot+ [mark=*, error bars/.cd, y dir=both,y explicit] coordinates {
            (11,2.15) +- (0.01,0.01)
            (37,2.49) +- (0.006,0.006)
            (63,2.84) +- (0.02,0.02)
            (90,3.21) +- (0.01,0.01)
            (116,3.62) +- (0.005,0.005)
            (145,4.11) +- (0.02,0.02)
            (174,4.62) +- (0.02,0.02)
            (203,5.23) +- (0.05,0.05)
            (232,5.72) +- (0.04,0.04)
        };  % gpu
        \legend{CPU ``set'',GPU ``set''}
        \end{axis}
        \end{tikzpicture}
        \caption{Time taken by the Neural Network: GPU Speedup over all tested batch-sizes varies between \textbf{0.28 $-$ 2.81} --- as transfer time between RAM and GPU's internal memory dominates for smaller datasets. However, as batch-size increases, computation time dominates over transfer time --- GPU ``set'' performs better than CPU ``set''.}
        \label{fig:Restricted Finding Rewards - Time taken by the Neural Network}
    \end{subfigure}\vspace*{1em}
    \begin{subfigure}{0.3\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
        xbar,
        width=0.9\textwidth,
        height=8cm,
        ytick=data,
        ylabel={Batch-Size $T$},
        xlabel={GPU Speedup},
        ytick align=inside,
        xmin=0,
        xmax=1.5,
        enlarge y limits=0.15,
        nodes near coords,
        every node near coord/.append style={/pgf/number format/.cd,fixed zerofill,precision=2},
        ]
        \addplot+ coordinates {
            (0.82,11)
            (0.95,37)
            (0.98,63)
            (0.99,90)
            (1.00,116)
            (1.03,145)
            (1.02,174)
            (1.02,203)
            (1.02,232)
        };
        \end{axis}
        \end{tikzpicture}
        \caption{Speedup for the Full Model}
        \label{fig:Restricted Finding Rewards - Speedup for the Full Model}
    \end{subfigure}\hfill
    \begin{subfigure}{0.3\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
        xbar,
        width=\textwidth,
        height=8cm,
        ytick=data,
        yticklabels={,,},
        xlabel={GPU Speedup},
        ytick align=inside,
        xmin=0,
        xmax=1.5,
        enlarge y limits=0.15,
        nodes near coords,
        every node near coord/.append style={/pgf/number format/.cd,fixed zerofill,precision=2},
        ]
        \addplot+ coordinates {
            (1.01,11)
            (1.00,37)
            (1.00,63)
            (0.99,90)
            (1.00,116)
            (1.02,145)
            (1.00,174)
            (1.00,203)
            (1.00,232)
        };
        \end{axis}
        \end{tikzpicture}
        \caption{Speedup for the LP}
        \label{fig:Restricted Finding Rewards - Speedup for LP}
    \end{subfigure}\hfill
    \begin{subfigure}{0.37\textwidth}
        \centering
        \begin{tikzpicture}
        \begin{axis}[
        xbar,
        width=\textwidth,
        height=8cm,
        ytick=data,
        yticklabels={,,},
        xlabel={GPU Speedup},
        ytick align=inside,
        xmin=0,
        xmax=3.75,
        enlarge y limits=0.15,
        nodes near coords,
        every node near coord/.append style={/pgf/number format/.cd,fixed zerofill,precision=2},
        ]
        \addplot+ coordinates {
            (0.28,11)
            (0.44,37)
            (0.65,63)
            (0.98,90)
            (1.30,116)
            (1.77,145)
            (2.04,174)
            (2.39,203)
            (2.81,232)
        };
        \end{axis}
        \end{tikzpicture}
        \caption{Speedup for the Neural Network}
        \label{fig:Restricted Finding Rewards - Speedup for the Neural Network}
    \end{subfigure}
    \caption[Finding Rewards After Restricing Script's Access to CPU Cores --- Execution Times of Different Batch-Sizes $J$ with GPU and CPU ``set'' Separately]{Finding Rewards After Restricing Script's Access to CPU Cores --- Execution Times of Different Batch-Sizes $J$ with GPU and CPU ``set'' Separately: Scaling is strongly hampered by the LP solver. Comparing the contributions of LP and Neural Network to total runtime on GPU ``set'', LP accounts for \textbf{94.42 $\pm$ 5.00}\% of the total time.}
    \label{fig:Restricted Finding Rewards - Execution Times of Different Batch-Sizes J with GPU and CPUa ``set'' Separately}
\end{figure}

\subsubsection{Final Results}
The results after restricting the script's access to a single CPU core are plotted in \Cref{fig:Restricted Finding Rewards - Execution Times of Different Batch-Sizes J with GPU and CPUa ``set'' Separately}. We prefer using the results after restricting access as it gives the better performance for CPU ``set''. The time elapsed for the Neural Network includes time taken to transfer tensors to and from the GPU, which results in overhead --- as seen in higher runtimes for smaller datasets in \Cref{fig:Restricted Finding Rewards - Time taken by the Neural Network,fig:Restricted Finding Rewards - Speedup for the Neural Network}. However, as the batch-size increases, we see computation time dominating over the transfer time, resulting in higher CPU ``set'' runtimes.

Moreover, the LP subproblem highly impacts the full model, accounting for more than $94\%$ of the total runtime. Therefore, even though the Neural Network gets sped up at bigger datasets, its GPU Speedup is not reflected in the full model.